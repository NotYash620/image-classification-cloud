# -*- coding: utf-8 -*-
"""YashKumar(23FE10CDS00398).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NpvpX1QSd8Umm_uRp9Mx8uRQE6GvfOTp

Implementation of a Deep Learning Model using PyTorch
Name: Yash
Roll No: 23FE10CDS00398
Course: Deep Learning (B.Tech â€“ Data Science, 5th Semester)

This notebook demonstrates an end-to-end implementation in PyTorch for image classification using the CIFAR-10 dataset. The pipeline includes data loading, preprocessing, model design (a small CNN), training, validation, evaluation, and saving the final model. Visualizations include sample images, loss/accuracy curves, confusion matrix, and classification report.

Objective: Build, train, and evaluate a Deep Learning model using PyTorch.
"""

import os
import random
import time
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import torchvision
import torchvision.transforms as transforms
from sklearn.metrics import confusion_matrix, classification_report


# For reproducibility
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)


print('PyTorch version:', torch.__version__)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print('Device:', device)

# Cell 5: Dataset loading and preprocessing
# Using CIFAR-10 (10 classes, small images 32x32)


# Define transforms for training and test
train_transform = transforms.Compose([
transforms.RandomHorizontalFlip(),
transforms.RandomCrop(32, padding=4),
transforms.ToTensor(),
transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],
std=[0.2023, 0.1994, 0.2010])
])


test_transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],
std=[0.2023, 0.1994, 0.2010])
])


# Download datasets (will be cached in Colab)
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)


# Split train into train+val
train_size = int(0.9 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_data, val_data = random_split(train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))


batch_size = 128
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)


classes = train_dataset.classes
print('Classes:', classes)
print('Train samples:', len(train_data), 'Val samples:', len(val_data), 'Test samples:', len(test_dataset))

# Cell 6: Visualize some samples from training set (un-normalized for display)
import matplotlib.pyplot as plt


def imshow(img, title=None):
    img = img.numpy().transpose((1, 2, 0))
    mean = np.array([0.4914, 0.4822, 0.4465])
    std = np.array([0.2023, 0.1994, 0.2010])
    img = std * img + mean
    img = np.clip(img, 0, 1)
    plt.imshow(img)
    if title:
        plt.title(title)
    plt.axis('off')


# get some random training images
dataiter = iter(train_loader)
images, labels = next(dataiter)


plt.figure(figsize=(10,4))
for i in range(8):
    plt.subplot(2,4,i+1)
    imshow(images[i], title=classes[labels[i]])
plt.suptitle('Sample training images (after augmentation)')
plt.show()

# Cell 7: Define a simple CNN
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2,2),


            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2,2),


            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2,2)
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128*4*4, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )


    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


model = SimpleCNN(num_classes=len(classes)).to(device)
print(model)

# Cell 8: Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

# Cell 9: Training loop
num_epochs = 35
train_losses = []
val_losses = []
train_accs = []
val_accs = []


for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for inputs, targets in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{num_epochs}'):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()


        running_loss += loss.item() * inputs.size(0)
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()


    epoch_loss = running_loss / total
    epoch_acc = 100. * correct / total
    train_losses.append(epoch_loss)
    train_accs.append(epoch_acc)


    # Validation
    model.eval()
    val_running_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_running_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            val_total += targets.size(0)f
            val_correct += predicted.eq(targets).sum().item()
    val_epoch_loss = val_running_loss / val_total
    val_epoch_acc = 100. * val_correct / val_total
    val_losses.append(val_epoch_loss)
    val_accs.append(val_epoch_acc)


    scheduler.step()


    print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f} Train Acc: {epoch_acc:.2f}% Val Loss: {val_epoch_loss:.4f} Val Acc: {val_epoch_acc:.2f}%')


print('Training finished')

# Cell 10: Plot training & validation loss curves
plt.figure(figsize=(8,5))
plt.plot(range(1,len(train_losses)+1), train_losses, label='Train Loss')
plt.plot(range(1,len(val_losses)+1), val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss curves')
plt.legend()
plt.show()

# Cell 11: Plot training & validation accuracy
plt.figure(figsize=(8,5))
plt.plot(range(1,len(train_accs)+1), train_accs, label='Train Acc')
plt.plot(range(1,len(val_accs)+1), val_accs, label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy curves')
plt.legend()
plt.show()

# Cell 12: Evaluate on test set and gather predictions
model.eval()
all_preds = []
all_targets = []
all_probs = []


with torch.no_grad():
    test_loss = 0.0
    test_total = 0
    test_correct = 0
    for inputs, targets in tqdm(test_loader, desc='Testing'):
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        test_loss += loss.item() * inputs.size(0)
        _, predicted = outputs.max(1)
        test_total += targets.size(0)
        test_correct += predicted.eq(targets).sum().item()
        all_preds.extend(predicted.cpu().numpy())
        all_targets.extend(targets.cpu().numpy())


test_loss = test_loss / test_total
test_acc = 100. * test_correct / test_total


print(f'Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}%')

# Cell 13: Confusion matrix
import seaborn as sns
cm = confusion_matrix(all_targets, all_preds)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Cell 14: Classification report
print('Classification Report:')
print(classification_report(all_targets, all_preds, target_names=classes, digits=4))

# Cell 15: Save the trained model
save_path = '23FE10CDS00398_Yash_DL_Assignment2_model.pth'
torch.save({
'model_state_dict': model.state_dict(),
'optimizer_state_dict': optimizer.state_dict(),
'classes': classes
}, save_path)
print('Model saved to', save_path)

torch.save(model.state_dict(), "23FE10CDS00398_Yash_FinalModel.pth")
